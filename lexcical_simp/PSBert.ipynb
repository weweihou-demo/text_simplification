{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "# Load the required submodules\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.tokenization\n",
    "from official.nlp.modeling import models\n",
    "\n",
    "import json\n",
    "\n",
    "max_len_mask = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_config.json',\n",
       " 'bert_model.ckpt.data-00000-of-00001',\n",
       " 'bert_model.ckpt.index',\n",
       " 'vocab.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "tf.io.gfile.listdir(gs_folder_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Set up tokenizer to generate Tensorflow dataset\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
    "     do_lower_case=True)\n",
    "\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase is connected with '_', single word is also masked with '_'\n",
    "def load_data(input_file, max_len_mask, max_seq_len, max_prediction_per_sequence, is_eval_data = True):\n",
    "    original_sents = []\n",
    "    masked_sents = []\n",
    "    vocab_positions = []\n",
    "    vocab_synomies = []\n",
    "    phrases = []\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_weights = []\n",
    "    idx = 1\n",
    "    sent_ids = []\n",
    "    with tf.io.gfile.GFile(input_file, \"r\") as reader:\n",
    "            while True:\n",
    "                line = reader.readline().strip()\n",
    "                if not line:\n",
    "                    break\n",
    "                parts = line.split('\\t')\n",
    "                sent = parts[0]\n",
    "                phrase = parts[1]\n",
    "                synomies = []\n",
    "                if is_eval_data:\n",
    "                    synomies = parts[3:]\n",
    "                \n",
    "                words = sent.split()\n",
    "                \n",
    "                for i in range(max_len_mask):\n",
    "                    #create a new sentence, then masks will replace the original phrases\n",
    "                    #e.g. This is the state_of_the_art method.\n",
    "                    # --> This is the [MASK] [MASK] [MASK] method.\n",
    "                    # --> This is the [MASK] [MASK] method.\n",
    "                    # --> This is the [MASK] method.\n",
    "                    new_tokens = []\n",
    "                    tokens = []\n",
    "                    for word in words:\n",
    "                        if word.__contains__('_'):\n",
    "                            if word.endswith('_'):\n",
    "                                tokens.append(word.replace('_', ''))\n",
    "                            else:\n",
    "                                sub_ps = word.split('_')\n",
    "                                tokens.extend(sub_ps)\n",
    "                            for x in range(i + 1):\n",
    "                                new_tokens.append('mm')\n",
    "                        else:\n",
    "                            new_tokens.append(word)\n",
    "                            tokens.append(word)\n",
    "                    tokens_a = tokenizer.tokenize(' '.join(new_tokens))\n",
    "                    tokens_b = tokenizer.tokenize(' '.join(tokens))\n",
    "                    \n",
    "                    if len(tokens_a) + len(tokens_b) > max_seq_len - 3:\n",
    "                        evg_len = int((max_seq_len - 3) / 2)\n",
    "                        tokens_a = tokens_a[:evg_len]\n",
    "                        tokens_b = tokens_b[:evg_len]\n",
    "                        \n",
    "                    seq_len = len(tokens_a) + len(tokens_b) + 3\n",
    "                    sub_ps = tokenizer.tokenize(phrase)\n",
    "                    window_size = len(sub_ps)\n",
    "                    phrase_ids = []\n",
    "                    i = 0\n",
    "                    while (i + window_size < len(tokens_b)):\n",
    "                        if tokens_b[i: i + window_size] == sub_ps:\n",
    "                            start = i + len(tokens_a) + 2\n",
    "                            end = start + window_size - 1\n",
    "                            phrase_ids.append((start, end))\n",
    "                        i += 1\n",
    "                    original_sents.append(tokens_b)\n",
    "                    vocab_positions.append(phrase_ids)\n",
    "                    phrases.append(phrase)\n",
    "                    if len(synomies) > 0:\n",
    "                        vocab_synomies.append(synomies)\n",
    "                    masked_tokens, masked_positions, masked_weights = create_masks(tokens_a, max_prediction_per_sequence)\n",
    "                    masked_sents.append(masked_tokens)\n",
    "                    masked_lm_weights.append(masked_weights)\n",
    "                    masked_lm_positions.append(masked_positions)\n",
    "                    sent_ids.append(str(idx)+'_'+str(i))\n",
    "        idx += 1\n",
    "    return masked_sents,masked_lm_positions, masked_lm_weights, original_sents, vocab_synomies, vocab_positions, phrases, sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase is connected with '_', single word is also masked with '_'\n",
    "def load_data(input_file, max_len_mask, max_seq_len, max_prediction_per_sequence, is_eval_data = True):\n",
    "    original_sents = []\n",
    "    masked_sents = []\n",
    "    vocab_positions = []\n",
    "    vocab_synomies = []\n",
    "    phrases = []\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_weights = []\n",
    "    idx = 1\n",
    "    sent_ids = []\n",
    "    with tf.io.gfile.GFile(input_file, \"r\") as reader:\n",
    "            while True:\n",
    "                line = reader.readline().strip()\n",
    "                if not line:\n",
    "                    break\n",
    "                parts = line.split('\\t')\n",
    "                sent = parts[0]\n",
    "                phrase = parts[1]\n",
    "                synomies = []\n",
    "                if is_eval_data:\n",
    "                    synomies = parts[3:]\n",
    "                \n",
    "                words = sent.split()\n",
    "                original_tokens = []\n",
    "                for word in words:\n",
    "                    if word.__contains__('_'):\n",
    "                        if word.endswith('_'):\n",
    "                            original_tokens.append(word.replace('_', ''))\n",
    "                        else:\n",
    "                            sub_ps = word.split('_')\n",
    "                            original_tokens.extend(sub_ps)\n",
    "                    else:\n",
    "                        original_tokens.append(word)\n",
    "                        \n",
    "                orginal_sent = tokenizer.tokenize(' '.join(original_tokens))\n",
    "                sub_ps = tokenizer.tokenize(phrase)\n",
    "                window_size = len(sub_ps)\n",
    "                phrase_ids = []\n",
    "                i = 0\n",
    "                while(i + window_size < len(orginal_sent)):\n",
    "                    if orginal_sent[i:i + window_size] == sub_ps:\n",
    "                        start = i \n",
    "                        end = start + window_size - 1\n",
    "                        phrase_ids.append((start, end))\n",
    "                    i += 1\n",
    "                #record orginal sentence and its phrase positions, the sentence index is the array index    \n",
    "                original_sents.append((tokens,phrase_ids, synomies))\n",
    "                \n",
    "                for i in range(max_len_mask):\n",
    "                    #create a new sentence, then masks will replace the original phrases\n",
    "                    #e.g. This is the state_of_the_art method.\n",
    "                    # --> This is the [MASK] [MASK] [MASK] method.\n",
    "                    # --> This is the [MASK] [MASK] method.\n",
    "                    # --> This is the [MASK] method.\n",
    "                    new_tokens = []\n",
    "                    tokens = []\n",
    "                    for word in words:\n",
    "                        if word.__contains__('_'):\n",
    "                            for x in range(i + 1):\n",
    "                                new_tokens.append('mm')\n",
    "                        else:\n",
    "                            new_tokens.append(word)\n",
    "                            \n",
    "                    tokens_a = tokenizer.tokenize(' '.join(new_tokens))\n",
    "                    \n",
    "                    masked_tokens, masked_positions, masked_weights = create_masks(tokens_a, max_prediction_per_sequence)\n",
    "                    masked_sents.append(masked_tokens)\n",
    "                    masked_lm_weights.append(masked_weights)\n",
    "                    masked_lm_positions.append(masked_positions)\n",
    "                    sent_ids.append(str(idx)+'_'+str(i))\n",
    "        idx += 1\n",
    "    return masked_sents,masked_lm_positions, masked_lm_weights, original_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(tokens, max_prediction_per_sequence):\n",
    "    idx = 1\n",
    "    masked_positions = []\n",
    "    masked_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == 'mm':\n",
    "            masked_tokens.append('[MASK]')\n",
    "            masked_positions.append(idx)\n",
    "        else:\n",
    "            masked_tokens.append(token)\n",
    "        idx += 1\n",
    "        \n",
    "    masked_positions.sort()\n",
    "    masked_lm_positions = masked_positions\n",
    "    masked_lm_weights = [1]*len(masked_positions)\n",
    "    #print(masked_tokens)\n",
    "    assert len(masked_positions) <= max_prediction_per_sequence\n",
    "    \n",
    "    paddings = max_prediction_per_sequence - len(masked_positions)\n",
    "    masked_lm_positions.extend([0]*(paddings))\n",
    "    masked_lm_weights.extend([0]*paddings)\n",
    "    return masked_tokens, masked_lm_positions, masked_lm_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(tokens):\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(masked_sents, \n",
    "                original_sents, \n",
    "                masked_lm_positions, \n",
    "                masked_lm_weights, \n",
    "                max_prediction_per_sequence):\n",
    "    \n",
    "    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(masked_sents)])\n",
    "    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(original_sents)])\n",
    "    print(sentence1.shape)\n",
    "    print(sentence2.shape)\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "    \n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s1 = tf.zeros_like(sentence1)\n",
    "    type_s2 = tf.ones_like(sentence2)\n",
    "    input_type_ids = tf.concat(\n",
    "      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    masked_lm_positions = tf.convert_to_tensor(masked_lm_positions, dtype=tf.int32)\n",
    "    masked_lm_weights = tf.convert_to_tensor(masked_lm_weights, dtype=tf.int32)\n",
    "    masked_lm_ids = tf.zeros_like(masked_lm_weights)\n",
    "    \n",
    "    inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids, \n",
    "      'masked_lm_weights': masked_lm_weights, \n",
    "      'masked_lm_positions': masked_lm_positions,\n",
    "      'masked_lm_ids':masked_lm_ids,\n",
    "    }\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(substitute, original):\n",
    "    ps_a = np.mean(substitute, axis=0)\n",
    "    ps_b = np.mean(substitute, axis=0)\n",
    "    \n",
    "    return consine_similarity(ps_a, ps_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tokens(tokens):\n",
    "    len_phrase = len(tokens[0])\n",
    "    all_phrases = []\n",
    "    for n in range(len_phrase):\n",
    "        phrase = []\n",
    "        for token_parts in tokens:\n",
    "            phrase.append(token_parts[n])\n",
    "        all_phrases.append(phrase)\n",
    "    return all_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEntity():\n",
    "    def __init__(self, sentence, phrase_position, sent_id):\n",
    "        self.sentence = sentence\n",
    "        self.phrase_position = phrase_position\n",
    "        self.sent_id = sent_id\n",
    "    \n",
    "    def get_input(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    input_file = '/home/weiwei/lexical_simplification/BenchPS.txt'\n",
    "    max_prediction_per_sequence = 5\n",
    "    max_seq_len = 64\n",
    "    max_len_mask = 3\n",
    "    top_k = 10\n",
    "    masked_sents, masked_lm_positions, masked_lm_weights, original_sents, vocab_synomies, vocab_positions, phrases, sent_ids = load_data(\n",
    "        input_file = input_file, max_len_mask = max_len_mask,\n",
    "        max_seq_len = max_seq_len, max_prediction_per_sequence = max_prediction_per_sequence)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for i in range(5):\n",
    "#         print(masked_sents[i])\n",
    "    \n",
    "    inputs = bert_encode(masked_sents, original_sents, masked_lm_positions, masked_lm_weights, max_prediction_per_sequence = max_prediction_per_sequence)\n",
    "    \n",
    "    bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
    "    config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
    "    bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "    model, encoder, pretrained = bert.bert_models.pretrain_model(bert_config=bert_config, seq_length=max_seq_len,max_predictions_per_seq=max_prediction_per_sequence,  use_next_sentence_label=False, return_core_pretrainer_model=True)\n",
    "    \n",
    "    #lm_outputs = pretrain_model(bert_config=bert_config, seq_length=max_seq_len,max_predictions_per_seq=max_prediction_per_sequence)\n",
    "    predictions = pretrained.predict(inputs)\n",
    "    embeddings = encoder.predict(inputs)\n",
    "    results = predictions['masked_lm']\n",
    "    \n",
    "    masked_weights = inputs['masked_lm_weights']\n",
    "    idx = 0\n",
    "    \n",
    "    #record predicted words' indices\n",
    "    all_sents = []\n",
    "    #records all sentences\n",
    "    for result in results:\n",
    "        tokens = []\n",
    "        #all masks in a sentence, each sentence only has one complex phrase\n",
    "        for masked, weight in zip(result, masked_weights[idx]):\n",
    "            if weight == 1:\n",
    "                values, indices = tf.math.top_k(input=masked, k=top_k)\n",
    "                words = tokenizer.convert_ids_to_tokens(values)\n",
    "                tokens.append(words)\n",
    "        phrases = combine_tokens(tokens) \n",
    "        for phrase in phrases:\n",
    "            new_sent = masked_sents[idx]\n",
    "            n = 0\n",
    "            for token in phrases:\n",
    "                #replace the mask to be the predicted token\n",
    "                new_sent[masked_lm_positions[n]] = token\n",
    "                n += 1\n",
    "            all_sents.append(SentenceEntity(new_sent, masked_lm_positions[:len(phrase)],sent_ids[idx]))\n",
    "        idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, None)\n",
      "(1200, None)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
